{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "shared-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import importlib\n",
    "import models.transunet as vit\n",
    "import models.decoder_layers as decoder_layers\n",
    "import models.utils as ut\n",
    "import experiments.config as conf\n",
    "# import data_processing.dataset_synapse as dp \n",
    "import data_processing.data_parser as dp\n",
    "import data_processing.dataset_synapse as ds\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "source": [
    "## Environment Checkup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "functional-apache",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "spare-enough",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "marked-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dp)\n",
    "dr = dp.DataReader(\"/home/bouzidkenza1/TransUnet/data/synapse-tfrecords/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "perceived-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dr.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dataset.take(10):\n",
    "    print(sample[0].shape)\n",
    "    plt.imshow(sample[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "creative-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# data_dir = \"/home/bouzidkenza1/TransUnet/data/train_npz/\"\n",
    "# X, y = dp.load_data(data_dir, 2211)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "underlying-addiction",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "..\\models\\utils.py:84: UserWarning: Resizing position embeddings from 24 to 32\n  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "config = conf.get_b16_none()\n",
    "trans = vit.TransUnet(config)\n",
    "trans.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x22f9a113b70>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x22f9a113c88>,\n",
       " <tensorflow.python.keras.layers.core.Reshape at 0x22f9a1ea2e8>,\n",
       " <models.layers.AddPositionEmbs at 0x22f9a228eb8>,\n",
       " <models.layers.TransformerBlock at 0x22f9a2cb7f0>,\n",
       " <models.layers.TransformerBlock at 0x22f9a30c4a8>,\n",
       " <models.layers.TransformerBlock at 0x22fa0579898>,\n",
       " <models.layers.TransformerBlock at 0x22fa18a0470>,\n",
       " <models.layers.TransformerBlock at 0x22fa0655518>,\n",
       " <models.layers.TransformerBlock at 0x22fa1a151d0>,\n",
       " <models.layers.TransformerBlock at 0x22fa1a369b0>,\n",
       " <models.layers.TransformerBlock at 0x22fa1ae3be0>,\n",
       " <models.layers.TransformerBlock at 0x22fa1b52128>,\n",
       " <models.layers.TransformerBlock at 0x22fa1bd0710>,\n",
       " <models.layers.TransformerBlock at 0x22fa1c893c8>,\n",
       " <models.layers.TransformerBlock at 0x22fa1d0a320>,\n",
       " <tensorflow.python.keras.layers.normalization.LayerNormalization at 0x22fa3ce3f28>,\n",
       " <tensorflow.python.keras.layers.core.Reshape at 0x22fa3d29d30>,\n",
       " <models.layers.SegmentationHead at 0x22fa3d302e8>]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "trans.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "knowing-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batch = dataset.batch(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "balanced-equipment",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trans.model.fit(data_batch, epochs=3, batch_size=32, verbose=1)"
   ]
  },
  {
   "source": [
    "## Test Decoder CUP "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = ds.load_data(\"../data/train_npz/\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10, 512, 512, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "..\\models\\utils.py:84: UserWarning: Resizing position embeddings from 24 to 32\n  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(conf)\n",
    "importlib.reload(vit)\n",
    "config = conf.get_b16_cup()\n",
    "trans = vit.TransUnet(config)\n",
    "trans.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/1 [==============================] - 79s 79s/step - loss: 1.8634\n",
      "Wall time: 1min 19s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1843437f898>"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "%%time\n",
    "trans.model.fit(x=X, y=y, epochs=1, batch_size=32, verbose=1)"
   ]
  },
  {
   "source": [
    "## Comparison to B16_None"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "..\\models\\utils.py:84: UserWarning: Resizing position embeddings from 24 to 32\n  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(conf)\n",
    "importlib.reload(vit)\n",
    "config = conf.get_b16_none()\n",
    "trans = vit.TransUnet(config)\n",
    "trans.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "1/1 [==============================] - 44s 44s/step - loss: 1.6900\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 28s 28s/step - loss: 0.9971\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 27s 27s/step - loss: 0.5585\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2fb8a05d780>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "trans.model.fit(x=X, y=y, epochs=3, batch_size=32, verbose=1)"
   ]
  },
  {
   "source": [
    "## Test Resnet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50v2 = tf.keras.applications.ResNet50V2(include_top=False, input_shape=(512,512,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 512, 512, 3) dtype=float32 (created by layer 'input_7')>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "resnet50v2.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10, 512, 512, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = resnet50v2(X[0].reshape(1,512,512,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([1, 16, 16, 2048])"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet50v2.layers[2].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"resnet50v2\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_7 (InputLayer)            [(None, 512, 512, 3) 0                                            \n__________________________________________________________________________________________________\nconv1_pad (ZeroPadding2D)       (None, 518, 518, 3)  0           input_7[0][0]                    \n__________________________________________________________________________________________________\nconv1_conv (Conv2D)             (None, 256, 256, 64) 9472        conv1_pad[0][0]                  \n__________________________________________________________________________________________________\npool1_pad (ZeroPadding2D)       (None, 258, 258, 64) 0           conv1_conv[0][0]                 \n__________________________________________________________________________________________________\npool1_pool (MaxPooling2D)       (None, 128, 128, 64) 0           pool1_pad[0][0]                  \n__________________________________________________________________________________________________\nconv2_block1_preact_bn (BatchNo (None, 128, 128, 64) 256         pool1_pool[0][0]                 \n__________________________________________________________________________________________________\nconv2_block1_preact_relu (Activ (None, 128, 128, 64) 0           conv2_block1_preact_bn[0][0]     \n__________________________________________________________________________________________________\nconv2_block1_1_conv (Conv2D)    (None, 128, 128, 64) 4096        conv2_block1_preact_relu[0][0]   \n__________________________________________________________________________________________________\nconv2_block1_1_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block1_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv2_block1_1_relu (Activation (None, 128, 128, 64) 0           conv2_block1_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv2_block1_2_pad (ZeroPadding (None, 130, 130, 64) 0           conv2_block1_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv2_block1_2_conv (Conv2D)    (None, 128, 128, 64) 36864       conv2_block1_2_pad[0][0]         \n__________________________________________________________________________________________________\nconv2_block1_2_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block1_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv2_block1_2_relu (Activation (None, 128, 128, 64) 0           conv2_block1_2_bn[0][0]          \n__________________________________________________________________________________________________\nconv2_block1_0_conv (Conv2D)    (None, 128, 128, 256 16640       conv2_block1_preact_relu[0][0]   \n__________________________________________________________________________________________________\nconv2_block1_3_conv (Conv2D)    (None, 128, 128, 256 16640       conv2_block1_2_relu[0][0]        \n__________________________________________________________________________________________________\nconv2_block1_out (Add)          (None, 128, 128, 256 0           conv2_block1_0_conv[0][0]        \n                                                                 conv2_block1_3_conv[0][0]        \n__________________________________________________________________________________________________\nconv2_block2_preact_bn (BatchNo (None, 128, 128, 256 1024        conv2_block1_out[0][0]           \n__________________________________________________________________________________________________\nconv2_block2_preact_relu (Activ (None, 128, 128, 256 0           conv2_block2_preact_bn[0][0]     \n__________________________________________________________________________________________________\nconv2_block2_1_conv (Conv2D)    (None, 128, 128, 64) 16384       conv2_block2_preact_relu[0][0]   \n__________________________________________________________________________________________________\nconv2_block2_1_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block2_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv2_block2_1_relu (Activation (None, 128, 128, 64) 0           conv2_block2_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv2_block2_2_pad (ZeroPadding (None, 130, 130, 64) 0           conv2_block2_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv2_block2_2_conv (Conv2D)    (None, 128, 128, 64) 36864       conv2_block2_2_pad[0][0]         \n__________________________________________________________________________________________________\nconv2_block2_2_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block2_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv2_block2_2_relu (Activation (None, 128, 128, 64) 0           conv2_block2_2_bn[0][0]          \n__________________________________________________________________________________________________\nconv2_block2_3_conv (Conv2D)    (None, 128, 128, 256 16640       conv2_block2_2_relu[0][0]        \n__________________________________________________________________________________________________\nconv2_block2_out (Add)          (None, 128, 128, 256 0           conv2_block1_out[0][0]           \n                                                                 conv2_block2_3_conv[0][0]        \n__________________________________________________________________________________________________\nconv2_block3_preact_bn (BatchNo (None, 128, 128, 256 1024        conv2_block2_out[0][0]           \n__________________________________________________________________________________________________\nconv2_block3_preact_relu (Activ (None, 128, 128, 256 0           conv2_block3_preact_bn[0][0]     \n__________________________________________________________________________________________________\nconv2_block3_1_conv (Conv2D)    (None, 128, 128, 64) 16384       conv2_block3_preact_relu[0][0]   \n__________________________________________________________________________________________________\nconv2_block3_1_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block3_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv2_block3_1_relu (Activation (None, 128, 128, 64) 0           conv2_block3_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv2_block3_2_pad (ZeroPadding (None, 130, 130, 64) 0           conv2_block3_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv2_block3_2_conv (Conv2D)    (None, 64, 64, 64)   36864       conv2_block3_2_pad[0][0]         \n__________________________________________________________________________________________________\nconv2_block3_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv2_block3_2_relu (Activation (None, 64, 64, 64)   0           conv2_block3_2_bn[0][0]          \n__________________________________________________________________________________________________\nmax_pooling2d_9 (MaxPooling2D)  (None, 64, 64, 256)  0           conv2_block2_out[0][0]           \n__________________________________________________________________________________________________\nconv2_block3_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block3_2_relu[0][0]        \n__________________________________________________________________________________________________\nconv2_block3_out (Add)          (None, 64, 64, 256)  0           max_pooling2d_9[0][0]            \n                                                                 conv2_block3_3_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block1_preact_bn (BatchNo (None, 64, 64, 256)  1024        conv2_block3_out[0][0]           \n__________________________________________________________________________________________________\nconv3_block1_preact_relu (Activ (None, 64, 64, 256)  0           conv3_block1_preact_bn[0][0]     \n__________________________________________________________________________________________________\nconv3_block1_1_conv (Conv2D)    (None, 64, 64, 128)  32768       conv3_block1_preact_relu[0][0]   \n__________________________________________________________________________________________________\nconv3_block1_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block1_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block1_1_relu (Activation (None, 64, 64, 128)  0           conv3_block1_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block1_2_pad (ZeroPadding (None, 66, 66, 128)  0           conv3_block1_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block1_2_conv (Conv2D)    (None, 64, 64, 128)  147456      conv3_block1_2_pad[0][0]         \n__________________________________________________________________________________________________\nconv3_block1_2_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block1_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block1_2_relu (Activation (None, 64, 64, 128)  0           conv3_block1_2_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block1_0_conv (Conv2D)    (None, 64, 64, 512)  131584      conv3_block1_preact_relu[0][0]   \n__________________________________________________________________________________________________\nconv3_block1_3_conv (Conv2D)    (None, 64, 64, 512)  66048       conv3_block1_2_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block1_out (Add)          (None, 64, 64, 512)  0           conv3_block1_0_conv[0][0]        \n                                                                 conv3_block1_3_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block2_preact_bn (BatchNo (None, 64, 64, 512)  2048        conv3_block1_out[0][0]           \n__________________________________________________________________________________________________\nconv3_block2_preact_relu (Activ (None, 64, 64, 512)  0           conv3_block2_preact_bn[0][0]     \n__________________________________________________________________________________________________\nconv3_block2_1_conv (Conv2D)    (None, 64, 64, 128)  65536       conv3_block2_preact_relu[0][0]   \n__________________________________________________________________________________________________\nconv3_block2_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block2_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block2_1_relu (Activation (None, 64, 64, 128)  0           conv3_block2_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block2_2_pad (ZeroPadding (None, 66, 66, 128)  0           conv3_block2_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block2_2_conv (Conv2D)    (None, 64, 64, 128)  147456      conv3_block2_2_pad[0][0]         \n__________________________________________________________________________________________________\nconv3_block2_2_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block2_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block2_2_relu (Activation (None, 64, 64, 128)  0           conv3_block2_2_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block2_3_conv (Conv2D)    (None, 64, 64, 512)  66048       conv3_block2_2_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block2_out (Add)          (None, 64, 64, 512)  0           conv3_block1_out[0][0]           \n                                                                 conv3_block2_3_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block3_preact_bn (BatchNo (None, 64, 64, 512)  2048        conv3_block2_out[0][0]           \n__________________________________________________________________________________________________\nconv3_block3_preact_relu (Activ (None, 64, 64, 512)  0           conv3_block3_preact_bn[0][0]     \n__________________________________________________________________________________________________\nconv3_block3_1_conv (Conv2D)    (None, 64, 64, 128)  65536       conv3_block3_preact_relu[0][0]   \n__________________________________________________________________________________________________\nconv3_block3_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block3_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block3_1_relu (Activation (None, 64, 64, 128)  0           conv3_block3_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block3_2_pad (ZeroPadding (None, 66, 66, 128)  0           conv3_block3_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block3_2_conv (Conv2D)    (None, 64, 64, 128)  147456      conv3_block3_2_pad[0][0]         \n__________________________________________________________________________________________________\nconv3_block3_2_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block3_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block3_2_relu (Activation (None, 64, 64, 128)  0           conv3_block3_2_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block3_3_conv (Conv2D)    (None, 64, 64, 512)  66048       conv3_block3_2_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block3_out (Add)          (None, 64, 64, 512)  0           conv3_block2_out[0][0]           \n                                                                 conv3_block3_3_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block4_preact_bn (BatchNo (None, 64, 64, 512)  2048        conv3_block3_out[0][0]           \n__________________________________________________________________________________________________\nconv3_block4_preact_relu (Activ (None, 64, 64, 512)  0           conv3_block4_preact_bn[0][0]     \n__________________________________________________________________________________________________\nconv3_block4_1_conv (Conv2D)    (None, 64, 64, 128)  65536       conv3_block4_preact_relu[0][0]   \n__________________________________________________________________________________________________\nconv3_block4_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block4_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block4_1_relu (Activation (None, 64, 64, 128)  0           conv3_block4_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block4_2_pad (ZeroPadding (None, 66, 66, 128)  0           conv3_block4_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147456      conv3_block4_2_pad[0][0]         \n__________________________________________________________________________________________________\nconv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n__________________________________________________________________________________________________\nmax_pooling2d_10 (MaxPooling2D) (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n__________________________________________________________________________________________________\nconv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block4_out (Add)          (None, 32, 32, 512)  0           max_pooling2d_10[0][0]           \n                                                                 conv3_block4_3_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block1_preact_bn (BatchNo (None, 32, 32, 512)  2048        conv3_block4_out[0][0]           \n__________________________________________________________________________________________________\nconv4_block1_preact_relu (Activ (None, 32, 32, 512)  0           conv4_block1_preact_bn[0][0]     \n__________________________________________________________________________________________________\nconv4_block1_1_conv (Conv2D)    (None, 32, 32, 256)  131072      conv4_block1_preact_relu[0][0]   \n__________________________________________________________________________________________________\nconv4_block1_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block1_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block1_1_relu (Activation (None, 32, 32, 256)  0           conv4_block1_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block1_2_pad (ZeroPadding (None, 34, 34, 256)  0           conv4_block1_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block1_2_conv (Conv2D)    (None, 32, 32, 256)  589824      conv4_block1_2_pad[0][0]         \n__________________________________________________________________________________________________\nconv4_block1_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block1_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block1_2_relu (Activation (None, 32, 32, 256)  0           conv4_block1_2_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block1_0_conv (Conv2D)    (None, 32, 32, 1024) 525312      conv4_block1_preact_relu[0][0]   \n__________________________________________________________________________________________________\nconv4_block1_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block1_2_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block1_out (Add)          (None, 32, 32, 1024) 0           conv4_block1_0_conv[0][0]        \n                                                                 conv4_block1_3_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block2_preact_bn (BatchNo (None, 32, 32, 1024) 4096        conv4_block1_out[0][0]           \n__________________________________________________________________________________________________\nconv4_block2_preact_relu (Activ (None, 32, 32, 1024) 0           conv4_block2_preact_bn[0][0]     \n__________________________________________________________________________________________________\nconv4_block2_1_conv (Conv2D)    (None, 32, 32, 256)  262144      conv4_block2_preact_relu[0][0]   \n__________________________________________________________________________________________________\nconv4_block2_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block2_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block2_1_relu (Activation (None, 32, 32, 256)  0           conv4_block2_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block2_2_pad (ZeroPadding (None, 34, 34, 256)  0           conv4_block2_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block2_2_conv (Conv2D)    (None, 32, 32, 256)  589824      conv4_block2_2_pad[0][0]         \n__________________________________________________________________________________________________\nconv4_block2_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block2_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block2_2_relu (Activation (None, 32, 32, 256)  0           conv4_block2_2_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block2_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block2_2_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block2_out (Add)          (None, 32, 32, 1024) 0           conv4_block1_out[0][0]           \n                                                                 conv4_block2_3_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block3_preact_bn (BatchNo (None, 32, 32, 1024) 4096        conv4_block2_out[0][0]           \n__________________________________________________________________________________________________\nconv4_block3_preact_relu (Activ (None, 32, 32, 1024) 0           conv4_block3_preact_bn[0][0]     \n__________________________________________________________________________________________________\nconv4_block3_1_conv (Conv2D)    (None, 32, 32, 256)  262144      conv4_block3_preact_relu[0][0]   \n__________________________________________________________________________________________________\nconv4_block3_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block3_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block3_1_relu (Activation (None, 32, 32, 256)  0           conv4_block3_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block3_2_pad (ZeroPadding (None, 34, 34, 256)  0           conv4_block3_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block3_2_conv (Conv2D)    (None, 32, 32, 256)  589824      conv4_block3_2_pad[0][0]         \n__________________________________________________________________________________________________\nconv4_block3_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block3_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block3_2_relu (Activation (None, 32, 32, 256)  0           conv4_block3_2_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block3_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block3_2_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block3_out (Add)          (None, 32, 32, 1024) 0           conv4_block2_out[0][0]           \n                                                                 conv4_block3_3_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block4_preact_bn (BatchNo (None, 32, 32, 1024) 4096        conv4_block3_out[0][0]           \n__________________________________________________________________________________________________\nconv4_block4_preact_relu (Activ (None, 32, 32, 1024) 0           conv4_block4_preact_bn[0][0]     \n__________________________________________________________________________________________________\nconv4_block4_1_conv (Conv2D)    (None, 32, 32, 256)  262144      conv4_block4_preact_relu[0][0]   \n__________________________________________________________________________________________________\nconv4_block4_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block4_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block4_1_relu (Activation (None, 32, 32, 256)  0           conv4_block4_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block4_2_pad (ZeroPadding (None, 34, 34, 256)  0           conv4_block4_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block4_2_conv (Conv2D)    (None, 32, 32, 256)  589824      conv4_block4_2_pad[0][0]         \n__________________________________________________________________________________________________\nconv4_block4_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block4_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block4_2_relu (Activation (None, 32, 32, 256)  0           conv4_block4_2_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block4_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block4_2_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block4_out (Add)          (None, 32, 32, 1024) 0           conv4_block3_out[0][0]           \n                                                                 conv4_block4_3_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block5_preact_bn (BatchNo (None, 32, 32, 1024) 4096        conv4_block4_out[0][0]           \n__________________________________________________________________________________________________\nconv4_block5_preact_relu (Activ (None, 32, 32, 1024) 0           conv4_block5_preact_bn[0][0]     \n__________________________________________________________________________________________________\nconv4_block5_1_conv (Conv2D)    (None, 32, 32, 256)  262144      conv4_block5_preact_relu[0][0]   \n__________________________________________________________________________________________________\nconv4_block5_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block5_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block5_1_relu (Activation (None, 32, 32, 256)  0           conv4_block5_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block5_2_pad (ZeroPadding (None, 34, 34, 256)  0           conv4_block5_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block5_2_conv (Conv2D)    (None, 32, 32, 256)  589824      conv4_block5_2_pad[0][0]         \n__________________________________________________________________________________________________\nconv4_block5_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block5_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block5_2_relu (Activation (None, 32, 32, 256)  0           conv4_block5_2_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block5_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block5_2_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block5_out (Add)          (None, 32, 32, 1024) 0           conv4_block4_out[0][0]           \n                                                                 conv4_block5_3_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block6_preact_bn (BatchNo (None, 32, 32, 1024) 4096        conv4_block5_out[0][0]           \n__________________________________________________________________________________________________\nconv4_block6_preact_relu (Activ (None, 32, 32, 1024) 0           conv4_block6_preact_bn[0][0]     \n__________________________________________________________________________________________________\nconv4_block6_1_conv (Conv2D)    (None, 32, 32, 256)  262144      conv4_block6_preact_relu[0][0]   \n__________________________________________________________________________________________________\nconv4_block6_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block6_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block6_1_relu (Activation (None, 32, 32, 256)  0           conv4_block6_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block6_2_pad (ZeroPadding (None, 34, 34, 256)  0           conv4_block6_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  589824      conv4_block6_2_pad[0][0]         \n__________________________________________________________________________________________________\nconv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n__________________________________________________________________________________________________\nmax_pooling2d_11 (MaxPooling2D) (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n__________________________________________________________________________________________________\nconv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block6_out (Add)          (None, 16, 16, 1024) 0           max_pooling2d_11[0][0]           \n                                                                 conv4_block6_3_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block1_preact_bn (BatchNo (None, 16, 16, 1024) 4096        conv4_block6_out[0][0]           \n__________________________________________________________________________________________________\nconv5_block1_preact_relu (Activ (None, 16, 16, 1024) 0           conv5_block1_preact_bn[0][0]     \n__________________________________________________________________________________________________\nconv5_block1_1_conv (Conv2D)    (None, 16, 16, 512)  524288      conv5_block1_preact_relu[0][0]   \n__________________________________________________________________________________________________\nconv5_block1_1_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block1_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block1_1_relu (Activation (None, 16, 16, 512)  0           conv5_block1_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block1_2_pad (ZeroPadding (None, 18, 18, 512)  0           conv5_block1_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block1_2_conv (Conv2D)    (None, 16, 16, 512)  2359296     conv5_block1_2_pad[0][0]         \n__________________________________________________________________________________________________\nconv5_block1_2_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block1_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block1_2_relu (Activation (None, 16, 16, 512)  0           conv5_block1_2_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block1_0_conv (Conv2D)    (None, 16, 16, 2048) 2099200     conv5_block1_preact_relu[0][0]   \n__________________________________________________________________________________________________\nconv5_block1_3_conv (Conv2D)    (None, 16, 16, 2048) 1050624     conv5_block1_2_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block1_out (Add)          (None, 16, 16, 2048) 0           conv5_block1_0_conv[0][0]        \n                                                                 conv5_block1_3_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block2_preact_bn (BatchNo (None, 16, 16, 2048) 8192        conv5_block1_out[0][0]           \n__________________________________________________________________________________________________\nconv5_block2_preact_relu (Activ (None, 16, 16, 2048) 0           conv5_block2_preact_bn[0][0]     \n__________________________________________________________________________________________________\nconv5_block2_1_conv (Conv2D)    (None, 16, 16, 512)  1048576     conv5_block2_preact_relu[0][0]   \n__________________________________________________________________________________________________\nconv5_block2_1_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block2_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block2_1_relu (Activation (None, 16, 16, 512)  0           conv5_block2_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block2_2_pad (ZeroPadding (None, 18, 18, 512)  0           conv5_block2_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block2_2_conv (Conv2D)    (None, 16, 16, 512)  2359296     conv5_block2_2_pad[0][0]         \n__________________________________________________________________________________________________\nconv5_block2_2_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block2_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block2_2_relu (Activation (None, 16, 16, 512)  0           conv5_block2_2_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block2_3_conv (Conv2D)    (None, 16, 16, 2048) 1050624     conv5_block2_2_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block2_out (Add)          (None, 16, 16, 2048) 0           conv5_block1_out[0][0]           \n                                                                 conv5_block2_3_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block3_preact_bn (BatchNo (None, 16, 16, 2048) 8192        conv5_block2_out[0][0]           \n__________________________________________________________________________________________________\nconv5_block3_preact_relu (Activ (None, 16, 16, 2048) 0           conv5_block3_preact_bn[0][0]     \n__________________________________________________________________________________________________\nconv5_block3_1_conv (Conv2D)    (None, 16, 16, 512)  1048576     conv5_block3_preact_relu[0][0]   \n__________________________________________________________________________________________________\nconv5_block3_1_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block3_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block3_1_relu (Activation (None, 16, 16, 512)  0           conv5_block3_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block3_2_pad (ZeroPadding (None, 18, 18, 512)  0           conv5_block3_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block3_2_conv (Conv2D)    (None, 16, 16, 512)  2359296     conv5_block3_2_pad[0][0]         \n__________________________________________________________________________________________________\nconv5_block3_2_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block3_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block3_2_relu (Activation (None, 16, 16, 512)  0           conv5_block3_2_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block3_3_conv (Conv2D)    (None, 16, 16, 2048) 1050624     conv5_block3_2_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block3_out (Add)          (None, 16, 16, 2048) 0           conv5_block2_out[0][0]           \n                                                                 conv5_block3_3_conv[0][0]        \n__________________________________________________________________________________________________\npost_bn (BatchNormalization)    (None, 16, 16, 2048) 8192        conv5_block3_out[0][0]           \n__________________________________________________________________________________________________\npost_relu (Activation)          (None, 16, 16, 2048) 0           post_bn[0][0]                    \n==================================================================================================\nTotal params: 23,564,800\nTrainable params: 23,519,360\nNon-trainable params: 45,440\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet50v2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.load(\"R50+ViT-B_16.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Transformer/encoder_norm/bias',\n",
       " 'Transformer/encoder_norm/scale',\n",
       " 'Transformer/encoderblock_0/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_0/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_0/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_0/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_0/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_0/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_0/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_0/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_1/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_1/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_1/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_1/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_1/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_1/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_1/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_1/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_10/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_10/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_10/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_10/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_10/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_10/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_10/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_10/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_11/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_11/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_11/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_11/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_11/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_11/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_11/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_11/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_2/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_2/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_2/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_2/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_2/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_2/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_2/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_2/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_3/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_3/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_3/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_3/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_3/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_3/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_3/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_3/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_4/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_4/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_4/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_4/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_4/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_4/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_4/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_4/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_5/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_5/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_5/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_5/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_5/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_5/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_5/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_5/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_6/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_6/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_6/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_6/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_6/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_6/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_6/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_6/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_7/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_7/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_7/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_7/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_7/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_7/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_7/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_7/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_8/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_8/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_8/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_8/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_8/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_8/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_8/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_8/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/encoderblock_9/LayerNorm_0/bias',\n",
       " 'Transformer/encoderblock_9/LayerNorm_0/scale',\n",
       " 'Transformer/encoderblock_9/LayerNorm_2/bias',\n",
       " 'Transformer/encoderblock_9/LayerNorm_2/scale',\n",
       " 'Transformer/encoderblock_9/MlpBlock_3/Dense_0/bias',\n",
       " 'Transformer/encoderblock_9/MlpBlock_3/Dense_0/kernel',\n",
       " 'Transformer/encoderblock_9/MlpBlock_3/Dense_1/bias',\n",
       " 'Transformer/encoderblock_9/MlpBlock_3/Dense_1/kernel',\n",
       " 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/key/bias',\n",
       " 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/key/kernel',\n",
       " 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/out/bias',\n",
       " 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/out/kernel',\n",
       " 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/query/bias',\n",
       " 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/query/kernel',\n",
       " 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/value/bias',\n",
       " 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/value/kernel',\n",
       " 'Transformer/posembed_input/pos_embedding',\n",
       " 'block1/unit1/conv1/kernel',\n",
       " 'block1/unit1/conv2/kernel',\n",
       " 'block1/unit1/conv3/kernel',\n",
       " 'block1/unit1/conv_proj/kernel',\n",
       " 'block1/unit1/gn1/bias',\n",
       " 'block1/unit1/gn1/scale',\n",
       " 'block1/unit1/gn2/bias',\n",
       " 'block1/unit1/gn2/scale',\n",
       " 'block1/unit1/gn3/bias',\n",
       " 'block1/unit1/gn3/scale',\n",
       " 'block1/unit1/gn_proj/bias',\n",
       " 'block1/unit1/gn_proj/scale',\n",
       " 'block1/unit2/conv1/kernel',\n",
       " 'block1/unit2/conv2/kernel',\n",
       " 'block1/unit2/conv3/kernel',\n",
       " 'block1/unit2/gn1/bias',\n",
       " 'block1/unit2/gn1/scale',\n",
       " 'block1/unit2/gn2/bias',\n",
       " 'block1/unit2/gn2/scale',\n",
       " 'block1/unit2/gn3/bias',\n",
       " 'block1/unit2/gn3/scale',\n",
       " 'block1/unit3/conv1/kernel',\n",
       " 'block1/unit3/conv2/kernel',\n",
       " 'block1/unit3/conv3/kernel',\n",
       " 'block1/unit3/gn1/bias',\n",
       " 'block1/unit3/gn1/scale',\n",
       " 'block1/unit3/gn2/bias',\n",
       " 'block1/unit3/gn2/scale',\n",
       " 'block1/unit3/gn3/bias',\n",
       " 'block1/unit3/gn3/scale',\n",
       " 'block2/unit1/conv1/kernel',\n",
       " 'block2/unit1/conv2/kernel',\n",
       " 'block2/unit1/conv3/kernel',\n",
       " 'block2/unit1/conv_proj/kernel',\n",
       " 'block2/unit1/gn1/bias',\n",
       " 'block2/unit1/gn1/scale',\n",
       " 'block2/unit1/gn2/bias',\n",
       " 'block2/unit1/gn2/scale',\n",
       " 'block2/unit1/gn3/bias',\n",
       " 'block2/unit1/gn3/scale',\n",
       " 'block2/unit1/gn_proj/bias',\n",
       " 'block2/unit1/gn_proj/scale',\n",
       " 'block2/unit2/conv1/kernel',\n",
       " 'block2/unit2/conv2/kernel',\n",
       " 'block2/unit2/conv3/kernel',\n",
       " 'block2/unit2/gn1/bias',\n",
       " 'block2/unit2/gn1/scale',\n",
       " 'block2/unit2/gn2/bias',\n",
       " 'block2/unit2/gn2/scale',\n",
       " 'block2/unit2/gn3/bias',\n",
       " 'block2/unit2/gn3/scale',\n",
       " 'block2/unit3/conv1/kernel',\n",
       " 'block2/unit3/conv2/kernel',\n",
       " 'block2/unit3/conv3/kernel',\n",
       " 'block2/unit3/gn1/bias',\n",
       " 'block2/unit3/gn1/scale',\n",
       " 'block2/unit3/gn2/bias',\n",
       " 'block2/unit3/gn2/scale',\n",
       " 'block2/unit3/gn3/bias',\n",
       " 'block2/unit3/gn3/scale',\n",
       " 'block2/unit4/conv1/kernel',\n",
       " 'block2/unit4/conv2/kernel',\n",
       " 'block2/unit4/conv3/kernel',\n",
       " 'block2/unit4/gn1/bias',\n",
       " 'block2/unit4/gn1/scale',\n",
       " 'block2/unit4/gn2/bias',\n",
       " 'block2/unit4/gn2/scale',\n",
       " 'block2/unit4/gn3/bias',\n",
       " 'block2/unit4/gn3/scale',\n",
       " 'block3/unit1/conv1/kernel',\n",
       " 'block3/unit1/conv2/kernel',\n",
       " 'block3/unit1/conv3/kernel',\n",
       " 'block3/unit1/conv_proj/kernel',\n",
       " 'block3/unit1/gn1/bias',\n",
       " 'block3/unit1/gn1/scale',\n",
       " 'block3/unit1/gn2/bias',\n",
       " 'block3/unit1/gn2/scale',\n",
       " 'block3/unit1/gn3/bias',\n",
       " 'block3/unit1/gn3/scale',\n",
       " 'block3/unit1/gn_proj/bias',\n",
       " 'block3/unit1/gn_proj/scale',\n",
       " 'block3/unit2/conv1/kernel',\n",
       " 'block3/unit2/conv2/kernel',\n",
       " 'block3/unit2/conv3/kernel',\n",
       " 'block3/unit2/gn1/bias',\n",
       " 'block3/unit2/gn1/scale',\n",
       " 'block3/unit2/gn2/bias',\n",
       " 'block3/unit2/gn2/scale',\n",
       " 'block3/unit2/gn3/bias',\n",
       " 'block3/unit2/gn3/scale',\n",
       " 'block3/unit3/conv1/kernel',\n",
       " 'block3/unit3/conv2/kernel',\n",
       " 'block3/unit3/conv3/kernel',\n",
       " 'block3/unit3/gn1/bias',\n",
       " 'block3/unit3/gn1/scale',\n",
       " 'block3/unit3/gn2/bias',\n",
       " 'block3/unit3/gn2/scale',\n",
       " 'block3/unit3/gn3/bias',\n",
       " 'block3/unit3/gn3/scale',\n",
       " 'block3/unit4/conv1/kernel',\n",
       " 'block3/unit4/conv2/kernel',\n",
       " 'block3/unit4/conv3/kernel',\n",
       " 'block3/unit4/gn1/bias',\n",
       " 'block3/unit4/gn1/scale',\n",
       " 'block3/unit4/gn2/bias',\n",
       " 'block3/unit4/gn2/scale',\n",
       " 'block3/unit4/gn3/bias',\n",
       " 'block3/unit4/gn3/scale',\n",
       " 'block3/unit5/conv1/kernel',\n",
       " 'block3/unit5/conv2/kernel',\n",
       " 'block3/unit5/conv3/kernel',\n",
       " 'block3/unit5/gn1/bias',\n",
       " 'block3/unit5/gn1/scale',\n",
       " 'block3/unit5/gn2/bias',\n",
       " 'block3/unit5/gn2/scale',\n",
       " 'block3/unit5/gn3/bias',\n",
       " 'block3/unit5/gn3/scale',\n",
       " 'block3/unit6/conv1/kernel',\n",
       " 'block3/unit6/conv2/kernel',\n",
       " 'block3/unit6/conv3/kernel',\n",
       " 'block3/unit6/gn1/bias',\n",
       " 'block3/unit6/gn1/scale',\n",
       " 'block3/unit6/gn2/bias',\n",
       " 'block3/unit6/gn2/scale',\n",
       " 'block3/unit6/gn3/bias',\n",
       " 'block3/unit6/gn3/scale',\n",
       " 'block3/unit7/conv1/kernel',\n",
       " 'block3/unit7/conv2/kernel',\n",
       " 'block3/unit7/conv3/kernel',\n",
       " 'block3/unit7/gn1/bias',\n",
       " 'block3/unit7/gn1/scale',\n",
       " 'block3/unit7/gn2/bias',\n",
       " 'block3/unit7/gn2/scale',\n",
       " 'block3/unit7/gn3/bias',\n",
       " 'block3/unit7/gn3/scale',\n",
       " 'block3/unit8/conv1/kernel',\n",
       " 'block3/unit8/conv2/kernel',\n",
       " 'block3/unit8/conv3/kernel',\n",
       " 'block3/unit8/gn1/bias',\n",
       " 'block3/unit8/gn1/scale',\n",
       " 'block3/unit8/gn2/bias',\n",
       " 'block3/unit8/gn2/scale',\n",
       " 'block3/unit8/gn3/bias',\n",
       " 'block3/unit8/gn3/scale',\n",
       " 'block3/unit9/conv1/kernel',\n",
       " 'block3/unit9/conv2/kernel',\n",
       " 'block3/unit9/conv3/kernel',\n",
       " 'block3/unit9/gn1/bias',\n",
       " 'block3/unit9/gn1/scale',\n",
       " 'block3/unit9/gn2/bias',\n",
       " 'block3/unit9/gn2/scale',\n",
       " 'block3/unit9/gn3/bias',\n",
       " 'block3/unit9/gn3/scale',\n",
       " 'cls',\n",
       " 'conv_root/kernel',\n",
       " 'embedding/bias',\n",
       " 'embedding/kernel',\n",
       " 'gn_root/bias',\n",
       " 'gn_root/scale',\n",
       " 'head/bias',\n",
       " 'head/kernel',\n",
       " 'pre_logits/bias',\n",
       " 'pre_logits/kernel']"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "weights.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights.files\n",
    "['embedding/kernel', 'embedding/bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(21843,)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "weights['head/bias'].shape"
   ]
  },
  {
   "source": [
    "## Test Hybrid Mode"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "..\\models\\utils.py:84: UserWarning: Resizing position embeddings from 24 to 8\n  UserWarning,\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Layer weight shape (2, 2, 1024, 768) not compatible with provided weight shape (16, 16, 3, 768)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-03f6b77ddb54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_b16_hybrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtrans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTransUnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\insa\\5if\\KTH\\P4\\DL\\Project\\TransUnet\\models\\transunet.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         optimizer = tfa.optimizers.SGDW(\n",
      "\u001b[1;32md:\\insa\\5if\\KTH\\P4\\DL\\Project\\TransUnet\\models\\transunet.py\u001b[0m in \u001b[0;36mload_pretrained\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    116\u001b[0m         local_filepath = tf.keras.utils.get_file(\n\u001b[0;32m    117\u001b[0m             fname, origin, cache_subdir=\"weights\")\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\insa\\5if\\KTH\\P4\\DL\\Project\\TransUnet\\models\\utils.py\u001b[0m in \u001b[0;36mload_weights_numpy\u001b[1;34m(model, params_path)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;31m# print()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;31m# print(\"Hey\", np.array(source_weights).shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mmatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"layer\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\anaconda\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m   1871\u001b[0m           raise ValueError(\n\u001b[0;32m   1872\u001b[0m               \u001b[1;34m'Layer weight shape %s not compatible with provided weight '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1873\u001b[1;33m               'shape %s' % (ref_shape, weight.shape))\n\u001b[0m\u001b[0;32m   1874\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1875\u001b[0m         \u001b[0mweight_index\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Layer weight shape (2, 2, 1024, 768) not compatible with provided weight shape (16, 16, 3, 768)"
     ]
    }
   ],
   "source": [
    "importlib.reload(conf)\n",
    "importlib.reload(decoder_layers)\n",
    "importlib.reload(vit)\n",
    "importlib.reload(ut)\n",
    "config = conf.get_b16_hybrid()\n",
    "trans = vit.TransUnet(config)\n",
    "trans.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trans.model.fit(x=X, y=y, epochs=1, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
       "array([[1, 4],\n",
       "       [2, 5],\n",
       "       [3, 6]])>"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "x = tf.ragged.constant([1, 4])\n",
    "y = tf.constant([2, 5])\n",
    "z = tf.constant([3, 6])\n",
    "test = tf.stack([x, y, z])\n",
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3612jvsc74a57bd02762fd0ccae7a6827a0de0868563b3d499c815e35640ddddc3d2dc7e9a34dcb9",
   "display_name": "Python 3.6.12 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}